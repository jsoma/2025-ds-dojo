{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db903641",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize an empty list to store product data\n",
    "product_data = []\n",
    "\n",
    "# Define the function to scrape a single page\n",
    "async def scrape_page(page):\n",
    "    html = await page.content()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    \n",
    "    # Find all product elements on the page\n",
    "    products = soup.find_all(\"div\", class_=\"product_item\")\n",
    "    \n",
    "    for product in products:\n",
    "        # Extract relevant product details\n",
    "        name = product.find(\"dt\", class_=\"item_name\").get_text(strip=True) if product.find(\"dt\", class_=\"item_name\") else None\n",
    "        price = product.find(\"span\", class_=\"price01_default\").get_text(strip=True) if product.find(\"span\", class_=\"price01_default\") else None\n",
    "        tax = product.find(\"p\", class_=\"normal_price\").get_text(strip=True) if product.find(\"p\", class_=\"normal_price\") else None\n",
    "        image = product.find(\"img\")[\"src\"] if product.find(\"img\") else None\n",
    "        tags = product.find(\"ul\", class_=\"item_icon\").get_text(strip=True) if product.find(\"ul\", class_=\"item_icon\") else None\n",
    "        \n",
    "        # Append data to our list as a dictionary\n",
    "        product_data.append({\n",
    "            \"name\": name,\n",
    "            \"price\": price,\n",
    "            \"tax\": tax,\n",
    "            \"image_url\": f\"https://www.national-azabu.net{image}\" if image else None,  # Assuming image URL is relative\n",
    "            \"tags\": tags\n",
    "        })\n",
    "\n",
    "# Define the function to handle pagination\n",
    "async def scrape_all_pages():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False)\n",
    "        page = await browser.new_page()\n",
    "        \n",
    "        # Visit the first page\n",
    "        await page.goto(\"https://www.national-azabu.net/products/list?category_id=52\")\n",
    "        \n",
    "        # Scrape the first page\n",
    "        await scrape_page(page)\n",
    "        \n",
    "        # Handle pagination\n",
    "        while True:\n",
    "            try:\n",
    "                # Try clicking the \"Next\" button\n",
    "                await page.locator(\"a:has-text('Next')\").first.click(timeout=10000)\n",
    "                time.sleep(3)  # Wait for the next page to load\n",
    "                await scrape_page(page)\n",
    "            except:\n",
    "                print(\"No more pages to scrape.\")\n",
    "                break\n",
    "\n",
    "        # Convert the product data into a DataFrame\n",
    "        df = pd.DataFrame(product_data)\n",
    "        print(df.head())  # Display the first few rows of data\n",
    "        # Optionally save to CSV\n",
    "        df.to_csv(\"products_output.csv\", index=False)\n",
    "\n",
    "# Run the scraper\n",
    "await scrape_all_pages()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b64fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.read_csv(\"products_output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a5cb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "async with async_playwright() as playwright:\n",
    "    browser = await playwright.chromium.launch(headless=False)\n",
    "    context = await browser.new_context()\n",
    "    page = await context.new_page()\n",
    "\n",
    "    base_url = \"https://www.national-azabu.net\"\n",
    "    category_url = f\"{base_url}/products/list?category_id=52\"\n",
    "    current_page = 1\n",
    "    all_products = []\n",
    "\n",
    "    while True:\n",
    "        # Visit the current page\n",
    "        await page.goto(f\"{category_url}&pageno={current_page}\")\n",
    "        await page.wait_for_selector(\"div.product_item\")  # Wait until product items are loaded\n",
    "        content = await page.content()  # Get the page content\n",
    "        soup = BeautifulSoup(content, \"html.parser\")  # Parse the content with BeautifulSoup\n",
    "\n",
    "        # Extracting product details\n",
    "        product_items = soup.select(\"div.product_item\")\n",
    "        \n",
    "        for product in product_items:\n",
    "            name = product.select_one(\"dt.item_name\").get_text(strip=True)\n",
    "\n",
    "            # Check if the price exists to avoid 'NoneType' errors\n",
    "            price_element = product.select_one(\"p.price01_default\")\n",
    "            price = price_element.get_text(strip=True) if price_element else \"N/A\"\n",
    "\n",
    "            img_element = product.select_one(\"div.item_photo img\")\n",
    "            img_url = img_element[\"src\"] if img_element else \"N/A\"\n",
    "            img_url = \"https://www.national-azabu.net\" + img_url if img_url != \"N/A\" else \"N/A\"\n",
    "\n",
    "            # Add product details to list\n",
    "            all_products.append({\n",
    "                \"name\": name,\n",
    "                \"price\": price,\n",
    "                \"img_url\": img_url\n",
    "            })\n",
    "\n",
    "        # Check if there's a \"Next\" button for pagination\n",
    "        next_button = await page.query_selector('li.pagenation__item-next a')\n",
    "        if not next_button:\n",
    "            break  # No more pages, exit the loop\n",
    "\n",
    "        current_page += 1  # Increment the page number\n",
    "\n",
    "    await context.close()\n",
    "    await browser.close()\n",
    "\n",
    "    # Create a pandas dataframe from the scraped data\n",
    "    df = pd.DataFrame(all_products)\n",
    "    df.head()  # Display the dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a840c251",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf6137",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "workshop": {
   "title": "National Azabu Supermarket Scraper",
   "description": "Scraping product data from National Azabu",
   "order": 4
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}